{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb80e14d",
   "metadata": {},
   "source": [
    "# The FireworQS: Sorana Aurelia & Erica Sturm\n",
    "### Womanium 2024 QML Project: Quantum Machine Learning for Conspicuity Detection in Production\n",
    "### Notebook 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac67e44f",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "Foo\n",
    "\n",
    "*Disclaimer: any book, paper, article, or other reference material provided below are the independent opinions of the authors and do not constitute an endorsement. No conflict of interest is reported.*\n",
    "\n",
    "This Jupyter Notebook covers Task 0 (JAX + PennyLane) and Task 1 of the quantum machine learning challenge given by Fraunhofer. Tasks 2 and 3 and appendix material will be covered in subsequent Jupyter Notebooks. We make use of standard markdown notation as well as several HTML tags which may not render in the intended manner. However, these issues are merely cosmetic and will not alter the content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac7a02",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Glossary of terms, symbols, and acronyms](#glossary)\n",
    "    * Includes subsection for quick-reference hyperlinks\n",
    "* [Overview of software modules](#overview)\n",
    "    * PennyLane\n",
    "    * JAX\n",
    "    * PennyLane + JAX\n",
    "* [Guided Walkthrough of \"Using JAX with PennyLane\"](#JAXxPL)\n",
    "    * [PennyLane with NumPy](#PLxNP)\n",
    "    * [Gradient descent (jax.grad)](#jaxgrad)\n",
    "    * [Vectorization (jax.vmap)](#jaxvmap)\n",
    "    * [Just-in-time compilation (jax.jit)](#jaxjit)\n",
    "    * [Handling randomness](#jaxrand)\n",
    "* [The PennyLane Codebooks](#PLcodebooks)\n",
    "    * [Introduction to Quantum Computing](#intro_qc)\n",
    "    * [Single Qubit Gates](#1qb_gates)\n",
    "    * [Circuits with Many Qubits](#mqb_gates)\n",
    "* [Guided Walkthrough of \"The Variational Classifier\"](#var_classifer)\n",
    "* [References](#refs)\n",
    "* [About the team](#bios)\n",
    "\n",
    "Markdown text written in `block format` indicates either a computer code snippet or a keyword/code word that is being used in its technical capacity in Python/PennyLane/JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b696c9",
   "metadata": {},
   "source": [
    "## Glossary<a class=\"anchor\" id=\"glossary\"></a>\n",
    "* ML = machine learning\n",
    "* CML = classical machine learning\n",
    "* QML = quantum machine learning\n",
    "* jit = just-in-time\n",
    "* CV = continuous variable(s)\n",
    "\n",
    "#### Quick-links\n",
    "* [Pennylane documenation](https://docs.pennylane.ai/en/stable/)\n",
    "* [JAX documentation](https://jax.readthedocs.io/en/latest/#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463fda0a",
   "metadata": {},
   "source": [
    "## Overview of Software Modules<a class=\"anchor\" id=\"overview\"></a>\n",
    "### PennyLane\n",
    "[PennyLane](https://pennylane.ai/) is an opensource Python module created by [Xanadu](https://www.xanadu.ai/) for implementing quantum machine learning (QML) routines using differential programming (explained below). It is available for cross-platform installation and offers several quantum computing simulators in addition to being hardware agnostic with the alteration of a single line of code. We provide an overview of key features and use-cases in this project, but we strongly encourage the interested reader to try out the excellent PennyLane [tutorial](https://pennylane.ai/qml/demos/tutorial_qubit_rotation/) and [demos](https://pennylane.ai/qml/demonstrations/) with support from the [documentation](https://docs.pennylane.ai/en/stable/). Finally, there is a [PennyLane Discord server](https://discord.com/invite/paYuHUA5hE) where people from around the globe may interact and learn from one another!\n",
    "\n",
    "To import PennyLane: `import pennylane as qml`\n",
    "\n",
    "### JAX\n",
    "JAX is an opensource Python framework originally designed for classical machine learning (CML) so that the user could encode numerical programs and transformations. It is compatible with other Python libraries such as TensorFlow and PyTorch, and it also serves as a backend for other Python and R modules. From the [documentation](https://jax.readthedocs.io/en/latest/): \"JAX is a Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning...JAX provides a familiar NumPy-style API for ease of adoption by researchers and engineers.\" That is, JAX speeds up massive calculations of array-based numerical systems such as those in machine learning applications, in a manner akin to the hugely popular NumPy module. The JAX framework is agnostic to backend hardware.\n",
    "\n",
    "The speedups are the result of three main functionalities: auto-differentiation, just-in-time compilation of code, and vectorization and parallelization. The JAX [tutorials](https://jax.readthedocs.io/en/latest/tutorials.html) explains each of these primary functions in great detail. We also wish to draw attention to the [JAX sharp bits](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) as there are several extremely important nuances that a user must be aware of to use JAX successfully. In particular, **JAX also has a built-in version of NumPy that functions a bit differently than the standard NumPy module.** Lastly, we note the [FAQ](https://jax.readthedocs.io/en/latest/faq.html) page might be helpful for debugging.\n",
    "\n",
    "The name JAX was originally an acronym, but the meaning is no longer relevant.\n",
    "\n",
    "To import JAX: `import jax`  \n",
    "To import the JAX version of NumPy: `import jax.numpy as jnp`   \n",
    "\n",
    "### PennyLane + JAX\n",
    "Given that JAX was initially created and used by the classical machine learning (ML) community, it is not wholly surprising that it is also used in the growing QML community. The primary difference is that the latter makes use of qubit hardware which can leverage quantum mechanical phenomena such as superposition, entanglement, and interference.\n",
    "\n",
    "JAX enables the efficient compilation of PennyLane code as well as several other libraries for different hardware backends because under the hood it is using the [XLA](https://openxla.org/xla) (Accelerated Linear Algebra) library. We will not discuss the finer details of this module, but note its importance. It is also possible to compile PennyLane software to XLA, which enables a "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56379467",
   "metadata": {},
   "source": [
    "## Guided Walkthrough of \"Using JAX with PennyLane\"<a class=\"anchor\" id=\"JAXxPL\"></a>\n",
    "In this section we explain our journey through the tutorial titled [\"Using JAX with PennyLane\"](https://pennylane.ai/qml/demos/tutorial_jax_transformations/). This is the first task requested of us according to \"Project Overview\" (first bullet point) of the project statement. We believe this tutorial does an excellent job of introducing the reader to these ideas, and we recreate the tutorial here with additional commentary about our journey, highlighting the information we found most valuable.\n",
    "\n",
    "*Side note: Later, in the [The PennyLane Codebooks](#PLcodebooks) section, we do not copy-and-paste exact code examples as we do in this section. We have done this because this because the PennyLane 'Codercises' log our learning progress, whereas the material presented in this tutorial is not.*\n",
    "\n",
    "### PennyLane with NumPy<a class=\"anchor\" id=\"PLxNP\"></a>\n",
    "Before we explore the official JAX with PennyLane documentation, we note that it encourages the reader to go through the [basic tutorial](https://pennylane.ai/qml/demos/tutorial_qubit_rotation/) if they are new to PennyLane as it only uses vanilla NumPy. We will not recreate this particular tutorial, but we enumerate several important details that we learned while reading this page.\n",
    "* A `device` is a \"computational object that can apply quantum operations and return a measurement.\" They must be initialized prior to assignment using the `device()` function of PennyLane. Devices can be hardware or software simulators. ([Documentation](https://docs.pennylane.ai/en/stable/code/api/pennylane.device.html#pennylane.device))\n",
    "    * Several software simulators are natively available.\n",
    "* A `Qnode` is a quantum node is \"an abstract encapulation of a quantum function described by a quantum circuit. QNodes are bound to a particular quantum device.\" A quantum node may be constructed by the `QNode` class ([documentation](https://docs.pennylane.ai/en/stable/code/api/pennylane.QNode.html#pennylane.QNode)) or the `qnode()` decorator ([documentation](https://docs.pennylane.ai/en/stable/code/api/pennylane.qnode.html#pennylane.qnode)). For a Python function to be a a QNode, it must obey certain rules and restrictions.\n",
    "* PennyLane automatically executes both analytic and numerical methods of differentiation to compute function gradients. \n",
    "* When using the `jax.grad` function to compute gradients of a quantum circuit function, it returns a function of the derivative with respect to the (user-selected) arguments, be they one or many.\n",
    "* A variety of gradient-descent based optimizers are available for use. They accept a cost function and initial parameters and may have other keywords for the user to specify (eg: number of steps).\n",
    "\n",
    "We will showcase and discuss all of the previous points in the code demonstrations to follow.\n",
    "\n",
    "### Using JAX with PennyLane\n",
    "We will be walking through the demo article of the same name, available [here](https://pennylane.ai/qml/demos/tutorial_jax_transformations/). We will see all three of JAX's functionalities (gradient descent, parellelization, simulation compiling and optimization) and how JAX handles randomization. In what follows, we will take the code directly from the demo webpage, but any discussion we provide will either be written in surrounding markdown cells or in the code cells as comments with the initials `FQS`.\n",
    "\n",
    "We begin with two installation lines, which we must include (but may be optional for other users depending on their `environment`). If the reader already has PennyLane and JAX, they may skip or comment-out either or both of these cells. We have surpressed the outputs of these two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e009e804",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pennylane in /home/erica/anaconda3/lib/python3.11/site-packages (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0 in /home/erica/anaconda3/lib/python3.11/site-packages (from pennylane) (1.24.3)\n",
      "Requirement already satisfied: scipy in /home/erica/anaconda3/lib/python3.11/site-packages (from pennylane) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/erica/anaconda3/lib/python3.11/site-packages (from pennylane) (3.2.1)\n",
      "Requirement already satisfied: rustworkx in /home/erica/anaconda3/lib/python3.11/site-packages (from pennylane) (0.14.2)\n",
      "Requirement already satisfied: autograd in /home/erica/anaconda3/lib/python3.11/site-packages (from pennylane) (1.6.2)\n",
      "Requirement already satisfied: toml in /home/erica/anaconda3/lib/python3.11/site-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: appdirs in /home/erica/anaconda3/lib/python3.11/site-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: semantic-version>=2.7 in /home/erica/anaconda3/lib/python3.11/site-packages (from pennylane) (2.10.0)\n",
      "Requirement already satisfied: autoray>=0.6.11 in /home/erica/anaconda3/lib/python3.11/site-packages (from pennylane) (0.6.12)\n",
      "Requirement already satisfied: cachetools in /home/erica/anaconda3/lib/python3.11/site-packages (from pennylane) (5.4.0)\n",
      "Requirement already satisfied: pennylane-lightning>=0.37 in /home/erica/anaconda3/lib/python3.11/site-packages (from pennylane) (0.37.0)\n",
      "Requirement already satisfied: requests in /home/erica/anaconda3/lib/python3.11/site-packages (from pennylane) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in /home/erica/anaconda3/lib/python3.11/site-packages (from pennylane) (4.12.2)\n",
      "Requirement already satisfied: packaging in /home/erica/anaconda3/lib/python3.11/site-packages (from pennylane) (23.1)\n",
      "Requirement already satisfied: future>=0.15.2 in /home/erica/anaconda3/lib/python3.11/site-packages (from autograd->pennylane) (0.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/erica/anaconda3/lib/python3.11/site-packages (from requests->pennylane) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/erica/anaconda3/lib/python3.11/site-packages (from requests->pennylane) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/erica/anaconda3/lib/python3.11/site-packages (from requests->pennylane) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/erica/anaconda3/lib/python3.11/site-packages (from requests->pennylane) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd26bbc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jax in /home/erica/anaconda3/lib/python3.11/site-packages (0.4.30)\n",
      "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /home/erica/anaconda3/lib/python3.11/site-packages (from jax) (0.4.30)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /home/erica/anaconda3/lib/python3.11/site-packages (from jax) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.22 in /home/erica/anaconda3/lib/python3.11/site-packages (from jax) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum in /home/erica/anaconda3/lib/python3.11/site-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /home/erica/anaconda3/lib/python3.11/site-packages (from jax) (1.11.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d6b86f",
   "metadata": {},
   "source": [
    "Now that we have installed the required modules, we begin the demo proper with several import statements. The text notes that some warnings have been silenced, and we note that this demo uses the JAX version of NumPy (`jnp`) instead of the regular NumPy (`np`). The last line of the cell shows the initialization of a `device` object, where we will be using a classical software simulator called `default.qubit` device to run our calculations and two subsystems (`wires`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f29a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FQS: We had to modify the line `from jax.config import config` as it would not run.\n",
    "     Our modification is immediately below the original which has been commented out.\n",
    "\"\"\"\n",
    "\n",
    "# Added to silence some warnings.\n",
    "# from jax.config import config\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pennylane as qml\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c199491",
   "metadata": {},
   "source": [
    "The first part of this tutorial we learn how to execute gradient descent with JAX by determining the expectation value for the first `wire` (subsystem) of an entangled system. We must first create that entangled circuit, which is accomplished via the `@qml.qnode` decorator which encapsulates this Python function. The `circuit` function is now a Qnode which will then run on our device, `dev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be214c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FQS: the code here has not been altered.\n",
    "\n",
    "@qml.qnode(dev, interface=\"jax\")\n",
    "def circuit(param):\n",
    "    # These two gates represent our QML model.\n",
    "    qml.RX(param, wires=0)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "\n",
    "    # The expval here will be the \"cost function\" we try to minimize.\n",
    "    # Usually, this would be defined by the problem we want to solve,\n",
    "    # but for this example we'll just use a single PauliZ.\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f4081",
   "metadata": {},
   "source": [
    "In the previous cell, the `circuit` function (Qnode) will recieve a single float argument which will be the angle of rotation about the x-axis of the Bloch Sphere that the first wire (wire 0) will be rotated through (`RX`). This first qubit is now in a superposition state, and the overall system is still considered seperable. Then, a controlled not (`CNOT`) gate entangles the two, and we no longer have a seperable state. Finally, the expectation value of the of Pauli-Z operator is computed and returned. Below, we have a diagram of what this circuit would look like using the native PennyLane visualization functionality, as explained in this [blog post](https://pennylane.ai/blog/2021/05/how-to-visualize-quantum-circuits-in-pennylane/). We have set `param` to 0.1 for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc3dc38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ──RX(0.10)─╭●─┤  <Z>\n",
      "1: ───────────╰X─┤     \n"
     ]
    }
   ],
   "source": [
    "print(qml.draw(circuit)(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25689c8f",
   "metadata": {},
   "source": [
    "The demo now shows us how to execute the above circuit to obtain the observable. The output is a JAX object of the `DeviceArray` type, indicating that the calculation was done in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f85f12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Array(0.99244503, dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# FQS: the code here has not been altered.\n",
    "\n",
    "print(f\"Result: {repr(circuit(0.123))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d7cc6",
   "metadata": {},
   "source": [
    "### Using JAX for gradient descent: `jax.grad`<a class=\"anchor\" id=\"jaxgrad\"></a>\n",
    "\n",
    "We are now ready to see one of JAX's four main functionalities: computing gradients. Here, we would like to have JAX find the best value for `param` that would optimize the final result. In this case, that means minimize the observable. We will use `jax.grad` to do this.\n",
    "\n",
    "In the executable cell that follows, line 8 shows that we will be computing the gradient of the Qnode object, the `circuit` function. This `grad_circuit` object is a function--a gradient that can be called with an argument to evaluate the gradient at that given point. This is what happens in line 9: the gradient's value is computed at the point $\\frac{\\pi}{2}$.\n",
    "\n",
    "In line 13, an initial value for the angle is provided. This is an ansatz. \n",
    "\n",
    "The last thing we need for a machine learning algorithm is a cost function. But in this case, we do not have to define one, we can just use the circuit itself, as that is precisely what we are trying to optimize! Therefore, we compare each step of the gradient descent against its previous step to see if we are in fact optmizing (minimizing) the final output. This is how the body of the `for` loop works in lines 18 and 19. The `param` variable that the next step will use is updated by evaluating the gradient at the present value.\n",
    "\n",
    "For example: in the first iteration, `param` is 0.123. The gradient is evaluated here, and then `param` is updated by subtracting that gradient value (the slope) from 0.123. This process is repeated 100 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7846e4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Descent\n",
      "---------------\n",
      "grad_circuit(jnp.pi / 2): -1.000\n",
      "Initial param: 0.123\n",
      "Initial cost: 0.992\n",
      "Tuned param: 3.142\n",
      "Tuned cost: -1.000\n"
     ]
    }
   ],
   "source": [
    "# FQS: the code here has not been altered.\n",
    "\n",
    "print(\"\\nGradient Descent\")\n",
    "print(\"---------------\")\n",
    "\n",
    "# We use jax.grad here to transform our circuit method into one\n",
    "# that calcuates the gradient of the output relative to the input.\n",
    "\n",
    "grad_circuit = jax.grad(circuit)\n",
    "print(f\"grad_circuit(jnp.pi / 2): {grad_circuit(jnp.pi / 2):0.3f}\")\n",
    "\n",
    "# We can then use this grad_circuit function to optimize the parameter value\n",
    "# via gradient descent.\n",
    "param = 0.123 # Some initial value.\n",
    "\n",
    "print(f\"Initial param: {param:0.3f}\")\n",
    "print(f\"Initial cost: {circuit(param):0.3f}\")\n",
    "\n",
    "for _ in range(100): # Run for 100 steps.\n",
    "    param -= grad_circuit(param) # Gradient-descent update.\n",
    "\n",
    "print(f\"Tuned param: {param:0.3f}\")\n",
    "print(f\"Tuned cost: {circuit(param):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da0726",
   "metadata": {},
   "source": [
    "### Using JAX for parallelization: `jax.vmap`<a class=\"anchor\" id=\"jaxvmap\"></a>\n",
    "JAX can speedup the optimization of target parameters (like `param` in the previous example) by way of \"batching\" which is running batches of circuits instead of computing gradients as the latter can be costly on quantum hardware. This is accomplished by vectorizing our circuit using the `vmap` (vectorizing map), which takes a Qnode function which is the same `circuit` as above.\n",
    "\n",
    "Vecotrization or \"single instruction multiple data\" (SIMD) calculations allow the hardware to simultaneously execute the same instruction multiple times, each with a different single option from the data. In the code below, line 6 'vectorizes' our circuit, which is akin to copying the circuit several times. This is the single instruction. The mapping function `jax.vmap` returns a batched version of the function it recieved as an argument. This `vcircuit` function also now has the ability to associate each of the copies with an array element that corresponds to a datum.\n",
    "\n",
    "In line 11 three values are initialized. These are the \"multiple data.\" Each one of these data will be given to a \"copy\" of the instructions and all three will executed simultaneously--that is, in parallel in line 13.\n",
    "\n",
    "Recall that the `circuit` function only computes the expectation value of Pauli-Z operator, this is not where optimization occurs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "590b42c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Batching and Evolutionary Strategies\n",
      "------------------------------------\n",
      "Batched result: [0.52336595 0.99244503 0.84136092]\n"
     ]
    }
   ],
   "source": [
    "# FQS: the code here has not been altered.\n",
    "\n",
    "print(\"\\n\\nBatching and Evolutionary Strategies\")\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "# Create a vectorized version of our original circuit.\n",
    "vcircuit = jax.vmap(circuit)\n",
    "\n",
    "# Now, we call the ``vcircuit`` with multiple parameters at once and get back a\n",
    "# batch of expectations.\n",
    "# This examples runs 3 quantum circuits in parallel.\n",
    "batch_params = jnp.array([1.02, 0.123, -0.571])\n",
    "\n",
    "batched_results = vcircuit(batch_params)\n",
    "print(f\"Batched result: {batched_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d6c6c",
   "metadata": {},
   "source": [
    "In contrast, if we had wanted to test these three values from line 11 without the vectorization ability of `jax.vmap` we would have had to execute each one separately either as three separate calls to `circuit`. (This could also be accomplished by iterating through the parameters using a `for` loop.) This would take three times longer than the parallelized process since each calcuation runs in sequence, one after another.\n",
    "\n",
    "But how can this help us optimize the circuit? We don't need to optimize it three separate times, so how does vectorizing help? We use this idea in conjunction with the idea of an evolutionary strategy (ES). The demo cites [this article](https://arxiv.org/abs/2012.00101) for ES with quantum computing. In the abstract of this work, the authors note that these optimization algorithms are gradient free.\n",
    "\n",
    "First, the expected value is computed for each parameter. We did this in the previous cell, and the results are stored in the JAX array `batched_results`. As the circuit is its own cost function and we are trying to minimize the cost, the smallest number here is also the smallest cost and therefore the parameter that gave that result is given the greatest weight. Here, the best result is `0.523...` which corresponds to the input parameter `1.02` and this is closest to the optimal value, $\\pi$. We can now generate new parameter--without the gradient! However, instead of demonstrating the full process with only three trial values, the tutorial expands the example to one hundred trials. This is more realistic, as no ML algorithm would *ever* use such a small number as three.\n",
    "\n",
    "In the cell below, the tutorial starts by setting seeding the random number generator of JAX in line 6 to ensure consistency for all readers. In line 9, one hundred different initial guesses are created for the `param` JAX array, and their average is computed in line 10. This average and the variation `var` will be used later during the iterative procedure to compute the updated parameters in a weighted fashion that decreases as the algorithm gets closer to convergence.\n",
    "\n",
    "The `for` begins by vecotrizing the circuit for the \"single instruction\" and using the `params` array as the \"many data.\" The result of all of these parallel calculations are stored in the variable `costs` in line 17. The next few lines (19 thru 21) are how we determine which of the provided parameters provided the best results so that they can be weighted appropriately. The `mean` variable enables the fair comparison of all parameters against a common metric, and it is updated to reflect the new average. \n",
    "\n",
    "In line 24 the variance is decreased in a geometric manner with every iteration such that the range of the parameters is tightened. Otherwise, it would be more difficult to converge, as the input parameters would be spanning too much of the solution space. The new parameters for the next iteration are generated randomly with certain conditions. The first condition is that the tutorial here uses a random seed, as this is a teaching tool and consistency matters. More imporantly, the new parameters are sampled over a weighted distribution about the newly updated mean with a slightly tighter variance than the previous iteration. These weights skew more heavily to the better parameters of the previous iteration. After 200 steps/iterations, the results are given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f093cf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value: -0.078\n",
      "Initial cost: 0.997\n",
      "Final value: -3.139\n",
      "Final cost: -1.000\n"
     ]
    }
   ],
   "source": [
    "# FQS: the code here has not been altered.\n",
    "\n",
    "# Needed to do randomness with JAX.\n",
    "# For more info on how JAX handles randomness, see the documentation.\n",
    "# https://jax.readthedocs.io/en/latest/jax.random.html\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate our first set of samples.\n",
    "params = jax.random.normal(key, (100,))\n",
    "mean = jnp.average(params)\n",
    "var = 1.0\n",
    "print(f\"Initial value: {mean:0.3f}\")\n",
    "print(f\"Initial cost: {circuit(mean):0.3f}\")\n",
    "\n",
    "for _ in range(200):\n",
    "    # In this line, we run all 100 circuits in parallel.\n",
    "    costs = vcircuit(params)\n",
    "\n",
    "    # Use exp(-x) here since the costs could be negative.\n",
    "    weights = jnp.exp(-costs)\n",
    "    mean = jnp.average(params, weights=weights)\n",
    "\n",
    "    # We decrease the variance as we converge to a solution.\n",
    "    var = var * 0.97\n",
    "\n",
    "    # Split the PRNGKey to generate a new set of random samples.\n",
    "    key, split = jax.random.split(key)\n",
    "    params = jax.random.normal(split, (100,)) * var + mean\n",
    "\n",
    "print(f\"Final value: {mean:0.3f}\")\n",
    "print(f\"Final cost: {circuit(mean):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a4d2d",
   "metadata": {},
   "source": [
    "Compared to the initial value and cost, the final results are much better, almost identical to that of the more expensive gradient descent. It might be possible to achieve even better results with additional steps, or a different way to compute the next parameter trials, but that is not discussed in this tutorial.\n",
    "\n",
    "We conclude this section with a comment that other sources outside of this official JAX + PennyLane tutorial webpage point out that JAX has other ways to enable parallelization schemes such as the [`jax.pmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html) which is another mapping but has \"support for collective operations.\" We will continue with demo, but wanted to note that JAX of course has many more features than what this one webpage covers.\n",
    "\n",
    "### Using JAX for \"just in time\" (jit) compilation: `jax.jit`<a class=\"anchor\" id=\"jaxjit\"></a>\n",
    "JAX uses the [XLA](https://openxla.org/xla) (Accelerated Linear Algebra) open-source library under the hood as its compiler. It is this XLA library that enables JAX to work on different types of backends (it can also take encoded ML models from other sources like PyTorch and TensorFlow, but that is beyond the scope of this document). When running a JAX program for the first time, XLA executes several optimizations and so this first run will be a little slower. However, a local version of this optimized code will be cached and therefore subsequent runs will run faster. This is an attractive feature since it enables the user to run a program multiple times with different parameters. \n",
    "\n",
    "We note that caching programs sometimes has unintended side-effects, particularly if a user is expecting the code to behave in a manner consistent with the Python interpreter. This is the first point discussed in the [\"JAX - The Sharp Bits\"](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) documentation. Due to its importance, we repeat a warning they provide: \"It is not recommended to use iterators in any JAX function you want to jit or in any control-flow primiative.\"\n",
    "\n",
    "We now return to the main tutorial for an example of jit in action. This code block redefines the same `circuit` function we are already familiar with, but this time the `qnode` decorator in line 6 just above the function declaration has a new keyword, `interface.` From the `qnode` decorator [documentation](https://docs.pennylane.ai/en/stable/code/api/pennylane.qnode.html#pennylane.qnode) (NOT the QNode class), we see that this keyword tells the quantum node that we will be using JAX (in this example) to execute the backpropagation and it accepts and returns JAX array objects. If this keyword is not given, the `qnode` defaults to autograd which accepts and returns Python datatypes or NumPy array values.\n",
    "\n",
    "In line 13 we see that compiling the circuit with JAX is as easy as just using `jax.jit` in contrast to earlier compilations such as `jax.grad` or `jax.vmap`. In lines 21, 26, and 31 we see a new function called `block_until_ready()`. This subroutine is a bit beyond the scope of this introductory tutorial, but in basic terms, it lets the Python interpreter continue running the rest of the cell without waiting for the current line of code to finish executing by returning a placeholder that will be overwritten later. We encourage the interested reader to examine the JAX documentation about [asynchronus dispatch](https://jax.readthedocs.io/en/latest/async_dispatch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4505b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Jit Example\n",
      "-----------\n",
      "No jit time: 0.0039 seconds\n",
      "First run time: 0.0313 seconds\n",
      "Second run time: 0.0001 seconds\n"
     ]
    }
   ],
   "source": [
    "# FQS: the code here has not been altered\n",
    "\n",
    "print(\"\\n\\nJit Example\")\n",
    "print(\"-----------\")\n",
    "\n",
    "@qml.qnode(dev, interface=\"jax\")\n",
    "def circuit(param):\n",
    "    qml.RX(param, wires=0)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# Compiling your circuit with JAX is very easy, just add jax.jit!\n",
    "jit_circuit = jax.jit(circuit)\n",
    "\n",
    "import time\n",
    "\n",
    "# No jit.\n",
    "start = time.time()\n",
    "# JAX runs async, so .block_until_ready() blocks until the computation\n",
    "# is actually finished. You'll only need to use this if you're doing benchmarking.\n",
    "circuit(0.123).block_until_ready()\n",
    "no_jit_time = time.time() - start\n",
    "\n",
    "# First call with jit.\n",
    "start = time.time()\n",
    "jit_circuit(0.123).block_until_ready()\n",
    "first_time = time.time() - start\n",
    "\n",
    "# Second call with jit.\n",
    "start = time.time()\n",
    "jit_circuit(0.123).block_until_ready()\n",
    "second_time = time.time() - start\n",
    "\n",
    "\n",
    "print(f\"No jit time: {no_jit_time:0.4f} seconds\")\n",
    "# Compilation overhead will make the first call slower than without jit...\n",
    "print(f\"First run time: {first_time:0.4f} seconds\")\n",
    "# ... but the second run time is >100x faster than the first!\n",
    "print(f\"Second run time: {second_time:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d975e9b",
   "metadata": {},
   "source": [
    "Even though the previous example is short, it is immediately clear that the first-time overhead due to the jit compiler is overcome by subsequent iterations. If one were to run the circuit again, we would still see an improved timing compared to the \"no jit time\" trial. We have provided one additional code block to do this, and encourage the reader to see how different initial guesses for `param` influence the timing by using their own values for `initial_guess` in line 3. Even with poorer initial guesses, the circuit is still faster than the no-jit-time trial! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25640be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIRD run time: 0.0008 seconds\n"
     ]
    }
   ],
   "source": [
    "# Third call with jit by the FireworQS team:\n",
    "\n",
    "initial_guess = 0.9\n",
    "\n",
    "start = time.time()\n",
    "jit_circuit(initial_guess).block_until_ready()\n",
    "third_time = time.time() - start\n",
    "\n",
    "print(f\"THIRD run time: {third_time:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4fd6c",
   "metadata": {},
   "source": [
    "### Handling randomization with JAX <a class=\"anchor\" id=\"jaxrand\"></a>\n",
    "This final section of the tutorial page is dedicated to how PennyLane and JAX handle the discrepancy between the pseudo-randomness of classical computation versus the true randomness of quantum computation. After all, a classical computer, even one simulating a quantum procedure, is never truly random and we can always choose to seed a \"random\" number generator to get identical results. In contrast, there is no way to seed a quantum computer; it is truly random.\n",
    "\n",
    "The tutorial notes that the following example will only apply if we are using `jax.jit`. If we do not use jit, then PennyLane will automatically reseed the random number generator for each execution. To use randomness with a method that will be \"jitted,\" the method must be constructed to reflect this, as they demonstrate below with `jax.random.PRNGKey` (PRNG stands for *pseudo-random number generator*).\n",
    "\n",
    "In line 11 we see the keyword `prng_key` which sets the random number seed or key. These values are provided later in lines 21 and 22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a38350cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Randomness\n",
      "----------\n",
      "key1: [ 1. -1. -1. -1.  1. -1. -1. -1.  1.  1.]\n",
      "key1: [ 1. -1. -1. -1.  1. -1. -1. -1.  1.  1.]\n",
      "key2: [-1. -1. -1. -1.  1. -1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "# FQS: the code here has not been altered\n",
    "\n",
    "print(\"\\n\\nRandomness\")\n",
    "print(\"----------\")\n",
    "\n",
    "# Let's create our circuit with randomness and compile it with jax.jit.\n",
    "@jax.jit\n",
    "def circuit(key, param):\n",
    "    # Notice how the device construction now happens within the jitted method.\n",
    "    # Also note the added '.jax' to the device path.\n",
    "    dev = qml.device(\"default.qubit.jax\", wires=2, shots=10, prng_key=key)\n",
    "\n",
    "    # Now we can create our qnode within the circuit function.\n",
    "    @qml.qnode(dev, interface=\"jax\", diff_method=None)\n",
    "    def my_circuit():\n",
    "        qml.RX(param, wires=0)\n",
    "        qml.CNOT(wires=[0, 1])\n",
    "        return qml.sample(qml.PauliZ(0))\n",
    "    return my_circuit()\n",
    "\n",
    "key1 = jax.random.PRNGKey(0)\n",
    "key2 = jax.random.PRNGKey(1)\n",
    "\n",
    "# Notice that the first two runs return exactly the same results,\n",
    "print(f\"key1: {circuit(key1, jnp.pi/2)}\")\n",
    "print(f\"key1: {circuit(key1, jnp.pi/2)}\")\n",
    "\n",
    "# The second run has different results.\n",
    "print(f\"key2: {circuit(key2, jnp.pi/2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09244af2",
   "metadata": {},
   "source": [
    "This concludes our review and walk-through of the [\"Using JAX with PennyLane\"](https://pennylane.ai/qml/demos/tutorial_jax_transformations/) demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f2ca8",
   "metadata": {},
   "source": [
    "## The PennyLane Codebooks<a class=\"anchor\" id=\"PLcodebooks\"></a>\n",
    "Now that we have reviewed the basics of PennyLane with JAX, we are ready to document our journey through the PennyLane Codebooks and their Codercises, a series of guided explanations. This corresponds to the first official requirement under \"Project Tasks/Deliverables\" on the second page of the project description. In contrast to [Guided Walkthrough of \"Using JAX with PennyLane\"](#JAXxPL), we will not be walking through every exercise with copied code. Instead, we will summarize what we learn. \n",
    "\n",
    "This task requires the completion of the following three codebooks: [\"Introduction to Quantum Computing\"](#intro_qc) (3 sections), [\"Single Qubit Gates\"](#1qb_gates) (7 sections), and [\"Circuits with Many Qubits\"](mqb_gates) (4 sections). Each section has several sub-sections with code exercises. EJS has successfully completed all associated \"Codercises\" in all three codebooks. This may be checked independently.\n",
    "\n",
    "Overall, the PennyLane codebooks do a phenomenal job with introducing quantum mechanical ideas alongside the PennyLane module. Neither topic introduces too much at once, and the coding challenges appropriately dovetail with the presented theory throughout. We do note however, that sometimes the error messages do not always serve to help the student, as they do not usually indicate where an error has occured, only that one did with a message that may not be relevant. As there is no way to access STDOUT, `print` statements also are of no use in this environment. To overcome this difficulty, one may create a standalone Python program or Jupyter Notebook to enable a more nuanced investigation. The authors wish to emphasize that this is an excellent learning tool for both quantum computing fundamentals and PennyLane, and that the lack of straightforward debugging functionality is a minor inconvenience.\n",
    "\n",
    "### Introduction to Quantum Computing<a class=\"anchor\" id=\"intro_qc\"></a>\n",
    "This section has three subsections: All About Qubits, Quantum Circuits, and Unitary Matrices. Essentially, this section introduces a subset of ideas from linear algebra mathematics over the complex field for a particular application, namely quantum mechanics. Definitions are given with examples using the notation that is ubiquitous not only for quantum computing, but for quantum mechanics in general. With the exception of quantum circuit diagrams, this material could be viewed as a general introduction to quantum science.\n",
    "\n",
    "Topics reviewed include the following:\n",
    "* Differences between classical bits and qubits.\n",
    "* Bra-ket notation and symbolic representation.\n",
    "* Matrix notation for statevectors and operators.\n",
    "* The definitions of bases, orthogonality, normalization, inner products, and unitarity.\n",
    "* Superposition, the Hadamard operation, and measurement.\n",
    "* Qunatum circuit diagrams, the meaning of circuit depth.\n",
    "* How to initialize, populate (single qubit gates only), and perform measurements a quantum circuit with PennyLane with the default simulator.\n",
    "\n",
    "All of the above topics had associated coding challenges for us to complete starting with NumPy only and then gently introducing fundamental elements of PennyLane. Exercises progressively ramped up in detail and requirements as the student becomes more comfortable with all aspects of the material. Repetition helps reinforce the most relevant keywords and ideas from PennyLane such as the the `qnode` decorator, or how to return the statevector at the end of a function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473a630",
   "metadata": {},
   "source": [
    "### Single Qubit Gates<a class=\"anchor\" id=\"1qb_gates\"></a>\n",
    "There are seven lessons in this section: X and H, It's Just a Phase, From a Different Angle, Universal Gate Sets, Prepare Yourself, Measurements, and What Did You Expect? While single qubit gates were shown in the Introduction, they are defined formally here, and their usage and behaivor is reinforced by repetition with the codercises. As before, the difficulty of these code challenges slowly increases in difficulty, requiring us to synthesize previous lessons with new material. This section (and the next one) emphasize the details of coding quantum circuits using the PennyLane module with only brief mentions of quantum mechanical theory such as phase. For example, the necessity of unitary operators is noted, but the deeper fundamental reasons are not discussed in great detail. Instead, the tutorials emphasize how one can maniupulate and use these quantum mechanical phenomena successfully.\n",
    "\n",
    "In this section we covered the following topics:\n",
    "* All single qubit gates.\n",
    "    * H, X, Y, Z, RX, RY, RZ.\n",
    "    * Special RZ cases: S, T\n",
    "    * Includes discussion of effects, matrix forms, PennyLane keywords, and circuit diagram representation.\n",
    "* What consitututes a universal gate set.\n",
    "* Superposition and how the Hadamard operation introduces it.\n",
    "* The difference between relative and global phase.\n",
    "* Introduction to the Bloch Sphere.\n",
    "* How to initialize a desired quantum state.\n",
    "* How to gather and interpret results.\n",
    "    * Projective measurements.\n",
    "    * The difference between measurements and observables.\n",
    "    * How to measure in different bases.\n",
    "* The definition of a unitary operation's adjoint and how to encode it.\n",
    "\n",
    "While some of the previous topics were first demonstrated in the Introduction Codebook, the review here contextualizes and deepens the reader's understanding, building on their foundation. For instance, the Hadamard gate was demonstrated in the Introduction, but here it is formally defined. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14779a6",
   "metadata": {},
   "source": [
    "### Circuits with Many Qubits<a class=\"anchor\" id=\"mqb_gates\"></a>\n",
    "This third section, and last that we will be discussing has four subsections: Multi-Qubit Systems, All Tied Up, We've Got it Under Control, and Multi-Qubit Gate Challenge. In a sense, this entire section could be viewed as an introduction to quantum entanglement--the basics and how to create and manipulate it.  As before, the goal here is not to develop a deep physics-based foundation for this esoteric phenomenon, but rather how one can use in quantum computing. This is done through a series of multi-qubit gate exercises.\n",
    "\n",
    "This section reviews the following topics:\n",
    "* Qubit ordering conventions (big versus little endian notation)\n",
    "* The mathematics of the tensor product.\n",
    "* The definitions of separability and entanglement.\n",
    "* Degree of entanglement using the Bell and GHZ states.\n",
    "* Controlled gates with two or more controls.\n",
    "    * Includes discussion of effects, matrix forms, PennyLane keywords, and circuit diagram representation.\n",
    "    * Toffoli and Fredkin gates defined.\n",
    "  \n",
    "This codebook concludes with a larger challenge to create a quantum multiplxer via a uniformly controlled rotation. That is, the exact state is created with the target circuit, including phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2f3f4",
   "metadata": {},
   "source": [
    "# This concludes Task 1.\n",
    "We have reviwed the JAX+PennyLane Tutorial (Task 0) and the three required codebooks on the PennyLane website (Task 1), documenting our learning process at all stages.\n",
    "\n",
    "The next tasks for the Variational Classifier (Task 2) and Quanvolutional Neural Networks (Task 3) are covered in separate notebooks to ensure that exercises that rely on different NumPy modules do not interfere with each other.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
